{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c78a8c7-c6f0-4fd3-a9ae-f25a8f1ce626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7034\n",
      "           1       1.00      1.00      1.00      2728\n",
      "\n",
      "    accuracy                           1.00      9762\n",
      "   macro avg       1.00      1.00      1.00      9762\n",
      "weighted avg       1.00      1.00      1.00      9762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "def extract_features(token, index):\n",
    "    return {\n",
    "        'token': token.lower(),\n",
    "        'is_capitalized': token[0].isupper(),\n",
    "        'is_all_caps': token.isupper(),\n",
    "        'is_title': token.istitle(),\n",
    "        'prefix-1': token[:1],\n",
    "        'prefix-2': token[:2],\n",
    "        'suffix-1': token[-1:],\n",
    "        'suffix-2': token[-2:]\n",
    "    }\n",
    "\n",
    "def load_training_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    X, y = [], []\n",
    "    for item in data:\n",
    "        tokens = item[\"tokens\"]\n",
    "        labels = item[\"labels\"]\n",
    "        if len(tokens) != len(labels):\n",
    "            continue  # skip inconsistent entries\n",
    "        for idx, token in enumerate(tokens):\n",
    "            X.append(extract_features(token, idx))\n",
    "            y.append(labels[idx])\n",
    "    return X, y\n",
    "\n",
    "# Load and prepare training data\n",
    "X_raw, y = load_training_data(\"name_training_data.json\")\n",
    "vec = DictVectorizer(sparse=False)\n",
    "X_vec = vec.fit_transform(X_raw)\n",
    "\n",
    "# Train and save model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_vec, y)\n",
    "\n",
    "joblib.dump(clf, \"name_model.joblib\")\n",
    "joblib.dump(vec, \"name_vectorizer.joblib\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_vec)\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "303acf84-6c77-459c-af20-2ded17dfa719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.             → NAME prob: 0.98 (Non-name: 0.02)\n",
      "CHARITHA        → NAME prob: 0.68 (Non-name: 0.32)\n",
      "SRI             → NAME prob: 0.82 (Non-name: 0.18)\n",
      "KUNAPAREDDY     → NAME prob: 0.94 (Non-name: 0.06)\n",
      "5               → NAME prob: 0.00 (Non-name: 1.00)\n",
      "years           → NAME prob: 0.00 (Non-name: 1.00)\n",
      "Predicted Name: Dr. CHARITHA SRI KUNAPAREDDY\n"
     ]
    }
   ],
   "source": [
    "import joblib  # or just `import joblib` if you're using `joblib` directly\n",
    "\n",
    "def predict_names(tokens):\n",
    "    clf = joblib.load(\"name_model.joblib\")       # Make sure this model supports predict_proba()\n",
    "    vec = joblib.load(\"name_vectorizer.joblib\")\n",
    "\n",
    "    features = [extract_features(tok, idx) for idx, tok in enumerate(tokens)]\n",
    "    X_vec = vec.transform(features)\n",
    "    probs = clf.predict_proba(X_vec)  # Get probabilities for each class\n",
    "\n",
    "    # Class 1 is usually the \"name\" label\n",
    "    for tok, prob in zip(tokens, probs):\n",
    "        print(f\"{tok:<15} → NAME prob: {prob[1]:.2f} (Non-name: {prob[0]:.2f})\")\n",
    "\n",
    "    # Get the most confident name predictions\n",
    "    name_tokens = [tok for tok, prob in zip(tokens, probs) if prob[1] > 0.5]\n",
    "    return \" \".join(name_tokens)\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"Dr.\", \"CHARITHA\", \"SRI\", \"KUNAPAREDDY\", \"5\", \"years\"]\n",
    "print(\"Predicted Name:\", predict_names(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8aa0e80-948b-4633-9736-e31099e2598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for mismatched tokens and labels...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def find_label_mismatches(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\"Checking for mismatched tokens and labels...\\n\")\n",
    "    for i, item in enumerate(data):\n",
    "        tokens = item.get(\"tokens\", [])\n",
    "        labels = item.get(\"labels\", [])\n",
    "        if len(tokens) != len(labels):\n",
    "            print(f\"❌ Mismatch at index {i}:\")\n",
    "            print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "            print(f\"  Labels ({len(labels)}): {labels}\\n\")\n",
    "\n",
    "find_label_mismatches(\"name_training_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84ca69fd-5872-4856-ade1-74e8853ad4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-compact JSON written to semi_compact_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'extra_ner_name_college_dataset.json'       # your unstructured file\n",
    "output_file = 'semi_compact_output.json'\n",
    "\n",
    "# Load the existing pretty-printed JSON\n",
    "with open(input_file, 'r') as infile:\n",
    "    data = json.load(infile)  # This must be a list of dicts\n",
    "\n",
    "# Custom writing logic\n",
    "with open(output_file, 'w') as outfile:\n",
    "    outfile.write('[\\n')\n",
    "    for idx, item in enumerate(data):\n",
    "        json_str = json.dumps(item, indent=None)\n",
    "        # Pretty-print with keys on separate lines\n",
    "        parsed = json.loads(json_str)\n",
    "        outfile.write('  {\\n')\n",
    "        for i, (k, v) in enumerate(parsed.items()):\n",
    "            comma = ',' if i < len(parsed) - 1 else ''\n",
    "            line = f'    \"{k}\": {json.dumps(v)}{comma}\\n'\n",
    "            outfile.write(line)\n",
    "        comma = ',' if idx < len(data) - 1 else ''\n",
    "        outfile.write(f'  }}{comma}\\n')\n",
    "    outfile.write(']\\n')\n",
    "\n",
    "print(f\"Semi-compact JSON written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc6ad37-e734-41c8-b426-3521f340ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Force PyTorch to avoid Keras issues\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "\n",
    "    name = []\n",
    "    education = []\n",
    "    skills = []\n",
    "\n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"]\n",
    "\n",
    "        if label == \"PER\":\n",
    "            name.append(word)\n",
    "        elif label == \"ORG\":\n",
    "            education.append(word)\n",
    "        elif label == \"MISC\":\n",
    "            skills.append(word)\n",
    "\n",
    "    return {\n",
    "        \"name\": \" \".join(name),\n",
    "        \"education_orgs\": list(set(education)),\n",
    "        \"skills\": list(set(skills))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdccbe3-04bf-4834-ba69-2ea3fab39d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '', 'education_orgs': ['MCA', 'Tri', '##dhartha Engineering College', 'Learning', 'FastAP', 'VR', 'Mahila Degree College', 'CHARITHA SRI KUNAP'], 'skills': ['SQL', 'Machine', 'Pandas', 'Python']}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "CHARITHA SRI KUNAPAREDDY\n",
    "Email: charitha@example.com\n",
    "\n",
    "Education:\n",
    "MCA from VR Siddhartha Engineering College, Andhra Pradesh\n",
    "B.Sc from Triveni Mahila Degree College\n",
    "\n",
    "Skills: Python, SQL, Machine Learning, Pandas, FastAPI\n",
    "\"\"\"\n",
    "\n",
    "result = extract_entities(sample_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591e60ab-6874-4ed9-b83f-0b024175e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "🔍 Results from: BERT-NER (dslim)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARITHA SRI KUN          → ORG\n",
      "MCA                       → ORG\n",
      "VR Siddhartha Engineering College → ORG\n",
      "Triveni Mahila Degree College → ORG\n",
      "Python                    → MISC\n",
      "S                         → MISC\n",
      "CS                        → MISC\n",
      "Java                      → MISC\n",
      "FastAP                    → MISC\n",
      "Salesforce Catalyst       → ORG\n",
      "\n",
      "==============================\n",
      "🔍 Results from: RoBERTa-NER (Jean-Baptiste)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Jean-Baptiste--roberta-large-ner-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARITHA SRI              → PER\n",
      " KUNAPAREDDY              → ORG\n",
      " charitha                 → PER\n",
      "sri                       → PER\n",
      " VR Siddhartha Engineering College → ORG\n",
      " Triveni Mahila Degree College → ORG\n",
      " Python                   → MISC\n",
      " SQL                      → MISC\n",
      " HTML                     → MISC\n",
      " CSS                      → MISC\n",
      " Java                     → MISC\n",
      " FastAPI                  → MISC\n",
      " Salesforce Catalyst\n",
      "     → ORG\n",
      "\n",
      "==============================\n",
      "🔍 Results from: Multilingual-BERT (Davlan)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCA                       → ORG\n",
      "VR Siddhartha Engineering College → ORG\n",
      "BS                        → ORG\n",
      "Triveni Mahila Degree College → ORG\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define models to compare\n",
    "model_names = {\n",
    "    \"BERT-NER (dslim)\": \"dslim/bert-base-NER\",\n",
    "    \"RoBERTa-NER (Jean-Baptiste)\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    \"Multilingual-BERT (Davlan)\": \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
    "}\n",
    "\n",
    "# Sample resume snippet (can be longer)\n",
    "resume_text = \"\"\"\n",
    "CHARITHA SRI KUNAPAREDDY\n",
    "Email: charitha.sri@example.com\n",
    "Phone: 9876543210\n",
    "\n",
    "Education:\n",
    "MCA - VR Siddhartha Engineering College\n",
    "BSc - Triveni Mahila Degree College\n",
    "\n",
    "Skills: Python, SQL, HTML, CSS, Java, FastAPI\n",
    "\n",
    "Experience:\n",
    "Intern at Salesforce Catalyst\n",
    "\"\"\"\n",
    "\n",
    "# Run inference across all models\n",
    "def extract_entities_from_model(model_name):\n",
    "    print(f\"\\n{'='*30}\\n🔍 Results from: {model_name}\\n{'='*30}\")\n",
    "    ner = pipeline(\n",
    "        \"token-classification\",\n",
    "        model=model_names[model_name],\n",
    "        aggregation_strategy=\"simple\",\n",
    "        framework=\"pt\"\n",
    "    )\n",
    "    entities = ner(resume_text)\n",
    "    for ent in entities:\n",
    "        print(f\"{ent['word']:25} → {ent['entity_group']}\")\n",
    "\n",
    "# Compare all models\n",
    "for model in model_names:\n",
    "    extract_entities_from_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f434c5-3e53-48dd-8565-3615a083b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# ---------- Load Hugging Face NER Model ----------\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# ---------- Extract Text from PDF ----------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "# ---------- Split Resume into Sections ----------\n",
    "def split_resume_sections(text):\n",
    "    sections = {}\n",
    "    current = \"general\"\n",
    "    sections[current] = []\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        clean = line.strip()\n",
    "        if not clean:\n",
    "            continue\n",
    "\n",
    "        # Match section headers\n",
    "        if re.match(r\"^(education|education details|experience|skills|projects|summary|objective|certifications?)[:\\s]*$\", clean.lower()):\n",
    "            current = clean.lower().strip(\": \")\n",
    "            sections[current] = []\n",
    "        else:\n",
    "            sections.setdefault(current, []).append(clean)\n",
    "\n",
    "    return sections\n",
    "\n",
    "# ---------- Apply NER to Header + Education ----------\n",
    "def extract_name_and_education_sectional(text):\n",
    "    sections = split_resume_sections(text)\n",
    "    \n",
    "    header_text = \"\\n\".join(sections.get(\"general\", [])[:5])  # top lines only\n",
    "    edu_text = \"\\n\".join(sections.get(\"education\", []))\n",
    "\n",
    "    entities_header = ner_pipeline(header_text)\n",
    "    entities_edu = ner_pipeline(edu_text)\n",
    "\n",
    "    name_tokens = [e['word'] for e in entities_header if e[\"entity_group\"] == \"PER\"]\n",
    "    org_tokens = [e['word'] for e in entities_edu if e[\"entity_group\"] == \"ORG\"]\n",
    "\n",
    "    name = \" \".join(name_tokens).strip()\n",
    "    education = list(set(org_tokens))\n",
    "\n",
    "    return name, education\n",
    "\n",
    "# ---------- Run the Pipeline ----------\n",
    "def parse_pdf_resume(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    name, education = extract_name_and_education_sectional(text)\n",
    "    \n",
    "    print(\"👤 Name:\", name)\n",
    "    print(\"🎓 Education Orgs:\", education)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf310)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
