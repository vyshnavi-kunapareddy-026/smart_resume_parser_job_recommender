{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c78a8c7-c6f0-4fd3-a9ae-f25a8f1ce626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7034\n",
      "           1       1.00      1.00      1.00      2728\n",
      "\n",
      "    accuracy                           1.00      9762\n",
      "   macro avg       1.00      1.00      1.00      9762\n",
      "weighted avg       1.00      1.00      1.00      9762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "def extract_features(token, index):\n",
    "    return {\n",
    "        'token': token.lower(),\n",
    "        'is_capitalized': token[0].isupper(),\n",
    "        'is_all_caps': token.isupper(),\n",
    "        'is_title': token.istitle(),\n",
    "        'prefix-1': token[:1],\n",
    "        'prefix-2': token[:2],\n",
    "        'suffix-1': token[-1:],\n",
    "        'suffix-2': token[-2:]\n",
    "    }\n",
    "\n",
    "def load_training_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    X, y = [], []\n",
    "    for item in data:\n",
    "        tokens = item[\"tokens\"]\n",
    "        labels = item[\"labels\"]\n",
    "        if len(tokens) != len(labels):\n",
    "            continue  # skip inconsistent entries\n",
    "        for idx, token in enumerate(tokens):\n",
    "            X.append(extract_features(token, idx))\n",
    "            y.append(labels[idx])\n",
    "    return X, y\n",
    "\n",
    "# Load and prepare training data\n",
    "X_raw, y = load_training_data(\"name_training_data.json\")\n",
    "vec = DictVectorizer(sparse=False)\n",
    "X_vec = vec.fit_transform(X_raw)\n",
    "\n",
    "# Train and save model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_vec, y)\n",
    "\n",
    "joblib.dump(clf, \"name_model.joblib\")\n",
    "joblib.dump(vec, \"name_vectorizer.joblib\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_vec)\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "303acf84-6c77-459c-af20-2ded17dfa719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.             ‚Üí NAME prob: 0.98 (Non-name: 0.02)\n",
      "CHARITHA        ‚Üí NAME prob: 0.68 (Non-name: 0.32)\n",
      "SRI             ‚Üí NAME prob: 0.82 (Non-name: 0.18)\n",
      "KUNAPAREDDY     ‚Üí NAME prob: 0.94 (Non-name: 0.06)\n",
      "5               ‚Üí NAME prob: 0.00 (Non-name: 1.00)\n",
      "years           ‚Üí NAME prob: 0.00 (Non-name: 1.00)\n",
      "Predicted Name: Dr. CHARITHA SRI KUNAPAREDDY\n"
     ]
    }
   ],
   "source": [
    "import joblib  # or just `import joblib` if you're using `joblib` directly\n",
    "\n",
    "def predict_names(tokens):\n",
    "    clf = joblib.load(\"name_model.joblib\")       # Make sure this model supports predict_proba()\n",
    "    vec = joblib.load(\"name_vectorizer.joblib\")\n",
    "\n",
    "    features = [extract_features(tok, idx) for idx, tok in enumerate(tokens)]\n",
    "    X_vec = vec.transform(features)\n",
    "    probs = clf.predict_proba(X_vec)  # Get probabilities for each class\n",
    "\n",
    "    # Class 1 is usually the \"name\" label\n",
    "    for tok, prob in zip(tokens, probs):\n",
    "        print(f\"{tok:<15} ‚Üí NAME prob: {prob[1]:.2f} (Non-name: {prob[0]:.2f})\")\n",
    "\n",
    "    # Get the most confident name predictions\n",
    "    name_tokens = [tok for tok, prob in zip(tokens, probs) if prob[1] > 0.5]\n",
    "    return \" \".join(name_tokens)\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"Dr.\", \"CHARITHA\", \"SRI\", \"KUNAPAREDDY\", \"5\", \"years\"]\n",
    "print(\"Predicted Name:\", predict_names(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8aa0e80-948b-4633-9736-e31099e2598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for mismatched tokens and labels...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def find_label_mismatches(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\"Checking for mismatched tokens and labels...\\n\")\n",
    "    for i, item in enumerate(data):\n",
    "        tokens = item.get(\"tokens\", [])\n",
    "        labels = item.get(\"labels\", [])\n",
    "        if len(tokens) != len(labels):\n",
    "            print(f\"‚ùå Mismatch at index {i}:\")\n",
    "            print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "            print(f\"  Labels ({len(labels)}): {labels}\\n\")\n",
    "\n",
    "find_label_mismatches(\"name_training_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84ca69fd-5872-4856-ade1-74e8853ad4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-compact JSON written to semi_compact_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'extra_ner_name_college_dataset.json'       # your unstructured file\n",
    "output_file = 'semi_compact_output.json'\n",
    "\n",
    "# Load the existing pretty-printed JSON\n",
    "with open(input_file, 'r') as infile:\n",
    "    data = json.load(infile)  # This must be a list of dicts\n",
    "\n",
    "# Custom writing logic\n",
    "with open(output_file, 'w') as outfile:\n",
    "    outfile.write('[\\n')\n",
    "    for idx, item in enumerate(data):\n",
    "        json_str = json.dumps(item, indent=None)\n",
    "        # Pretty-print with keys on separate lines\n",
    "        parsed = json.loads(json_str)\n",
    "        outfile.write('  {\\n')\n",
    "        for i, (k, v) in enumerate(parsed.items()):\n",
    "            comma = ',' if i < len(parsed) - 1 else ''\n",
    "            line = f'    \"{k}\": {json.dumps(v)}{comma}\\n'\n",
    "            outfile.write(line)\n",
    "        comma = ',' if idx < len(data) - 1 else ''\n",
    "        outfile.write(f'  }}{comma}\\n')\n",
    "    outfile.write(']\\n')\n",
    "\n",
    "print(f\"Semi-compact JSON written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc6ad37-e734-41c8-b426-3521f340ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Force PyTorch to avoid Keras issues\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "\n",
    "    name = []\n",
    "    education = []\n",
    "    skills = []\n",
    "\n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"]\n",
    "\n",
    "        if label == \"PER\":\n",
    "            name.append(word)\n",
    "        elif label == \"ORG\":\n",
    "            education.append(word)\n",
    "        elif label == \"MISC\":\n",
    "            skills.append(word)\n",
    "\n",
    "    return {\n",
    "        \"name\": \" \".join(name),\n",
    "        \"education_orgs\": list(set(education)),\n",
    "        \"skills\": list(set(skills))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdccbe3-04bf-4834-ba69-2ea3fab39d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '', 'education_orgs': ['MCA', 'Tri', '##dhartha Engineering College', 'Learning', 'FastAP', 'VR', 'Mahila Degree College', 'CHARITHA SRI KUNAP'], 'skills': ['SQL', 'Machine', 'Pandas', 'Python']}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "CHARITHA SRI KUNAPAREDDY\n",
    "Email: charitha@example.com\n",
    "\n",
    "Education:\n",
    "MCA from VR Siddhartha Engineering College, Andhra Pradesh\n",
    "B.Sc from Triveni Mahila Degree College\n",
    "\n",
    "Skills: Python, SQL, Machine Learning, Pandas, FastAPI\n",
    "\"\"\"\n",
    "\n",
    "result = extract_entities(sample_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591e60ab-6874-4ed9-b83f-0b024175e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üîç Results from: BERT-NER (dslim)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARITHA SRI KUN          ‚Üí ORG\n",
      "MCA                       ‚Üí ORG\n",
      "VR Siddhartha Engineering College ‚Üí ORG\n",
      "Triveni Mahila Degree College ‚Üí ORG\n",
      "Python                    ‚Üí MISC\n",
      "S                         ‚Üí MISC\n",
      "CS                        ‚Üí MISC\n",
      "Java                      ‚Üí MISC\n",
      "FastAP                    ‚Üí MISC\n",
      "Salesforce Catalyst       ‚Üí ORG\n",
      "\n",
      "==============================\n",
      "üîç Results from: RoBERTa-NER (Jean-Baptiste)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Jean-Baptiste--roberta-large-ner-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARITHA SRI              ‚Üí PER\n",
      " KUNAPAREDDY              ‚Üí ORG\n",
      " charitha                 ‚Üí PER\n",
      "sri                       ‚Üí PER\n",
      " VR Siddhartha Engineering College ‚Üí ORG\n",
      " Triveni Mahila Degree College ‚Üí ORG\n",
      " Python                   ‚Üí MISC\n",
      " SQL                      ‚Üí MISC\n",
      " HTML                     ‚Üí MISC\n",
      " CSS                      ‚Üí MISC\n",
      " Java                     ‚Üí MISC\n",
      " FastAPI                  ‚Üí MISC\n",
      " Salesforce Catalyst\n",
      "     ‚Üí ORG\n",
      "\n",
      "==============================\n",
      "üîç Results from: Multilingual-BERT (Davlan)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCA                       ‚Üí ORG\n",
      "VR Siddhartha Engineering College ‚Üí ORG\n",
      "BS                        ‚Üí ORG\n",
      "Triveni Mahila Degree College ‚Üí ORG\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define models to compare\n",
    "model_names = {\n",
    "    \"BERT-NER (dslim)\": \"dslim/bert-base-NER\",\n",
    "    \"RoBERTa-NER (Jean-Baptiste)\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    \"Multilingual-BERT (Davlan)\": \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
    "}\n",
    "\n",
    "# Sample resume snippet (can be longer)\n",
    "resume_text = \"\"\"\n",
    "CHARITHA SRI KUNAPAREDDY\n",
    "Email: charitha.sri@example.com\n",
    "Phone: 9876543210\n",
    "\n",
    "Education:\n",
    "MCA - VR Siddhartha Engineering College\n",
    "BSc - Triveni Mahila Degree College\n",
    "\n",
    "Skills: Python, SQL, HTML, CSS, Java, FastAPI\n",
    "\n",
    "Experience:\n",
    "Intern at Salesforce Catalyst\n",
    "\"\"\"\n",
    "\n",
    "# Run inference across all models\n",
    "def extract_entities_from_model(model_name):\n",
    "    print(f\"\\n{'='*30}\\nüîç Results from: {model_name}\\n{'='*30}\")\n",
    "    ner = pipeline(\n",
    "        \"token-classification\",\n",
    "        model=model_names[model_name],\n",
    "        aggregation_strategy=\"simple\",\n",
    "        framework=\"pt\"\n",
    "    )\n",
    "    entities = ner(resume_text)\n",
    "    for ent in entities:\n",
    "        print(f\"{ent['word']:25} ‚Üí {ent['entity_group']}\")\n",
    "\n",
    "# Compare all models\n",
    "for model in model_names:\n",
    "    extract_entities_from_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f434c5-3e53-48dd-8565-3615a083b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# ---------- Load Hugging Face NER Model ----------\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# ---------- Extract Text from PDF ----------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "# ---------- Split Resume into Sections ----------\n",
    "def split_resume_sections(text):\n",
    "    sections = {}\n",
    "    current = \"general\"\n",
    "    sections[current] = []\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        clean = line.strip()\n",
    "        if not clean:\n",
    "            continue\n",
    "\n",
    "        # Match section headers\n",
    "        if re.match(r\"^(education|education details|experience|skills|projects|summary|objective|certifications?)[:\\s]*$\", clean.lower()):\n",
    "            current = clean.lower().strip(\": \")\n",
    "            sections[current] = []\n",
    "        else:\n",
    "            sections.setdefault(current, []).append(clean)\n",
    "\n",
    "    return sections\n",
    "\n",
    "# ---------- Apply NER to Header + Education ----------\n",
    "def extract_name_and_education_sectional(text):\n",
    "    sections = split_resume_sections(text)\n",
    "    \n",
    "    header_text = \"\\n\".join(sections.get(\"general\", [])[:5])  # top lines only\n",
    "    edu_text = \"\\n\".join(sections.get(\"education\", []))\n",
    "\n",
    "    entities_header = ner_pipeline(header_text)\n",
    "    entities_edu = ner_pipeline(edu_text)\n",
    "\n",
    "    name_tokens = [e['word'] for e in entities_header if e[\"entity_group\"] == \"PER\"]\n",
    "    org_tokens = [e['word'] for e in entities_edu if e[\"entity_group\"] == \"ORG\"]\n",
    "\n",
    "    name = \" \".join(name_tokens).strip()\n",
    "    education = list(set(org_tokens))\n",
    "\n",
    "    return name, education\n",
    "\n",
    "# ---------- Run the Pipeline ----------\n",
    "def parse_pdf_resume(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    name, education = extract_name_and_education_sectional(text)\n",
    "    \n",
    "    print(\"üë§ Name:\", name)\n",
    "    print(\"üéì Education Orgs:\", education)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf310)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
