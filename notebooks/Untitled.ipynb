{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c78a8c7-c6f0-4fd3-a9ae-f25a8f1ce626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      7034\n",
      "           1       1.00      1.00      1.00      2728\n",
      "\n",
      "    accuracy                           1.00      9762\n",
      "   macro avg       1.00      1.00      1.00      9762\n",
      "weighted avg       1.00      1.00      1.00      9762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "def extract_features(token, index):\n",
    "    return {\n",
    "        'token': token.lower(),\n",
    "        'is_capitalized': token[0].isupper(),\n",
    "        'is_all_caps': token.isupper(),\n",
    "        'is_title': token.istitle(),\n",
    "        'prefix-1': token[:1],\n",
    "        'prefix-2': token[:2],\n",
    "        'suffix-1': token[-1:],\n",
    "        'suffix-2': token[-2:]\n",
    "    }\n",
    "\n",
    "def load_training_data(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    X, y = [], []\n",
    "    for item in data:\n",
    "        tokens = item[\"tokens\"]\n",
    "        labels = item[\"labels\"]\n",
    "        if len(tokens) != len(labels):\n",
    "            continue  # skip inconsistent entries\n",
    "        for idx, token in enumerate(tokens):\n",
    "            X.append(extract_features(token, idx))\n",
    "            y.append(labels[idx])\n",
    "    return X, y\n",
    "\n",
    "# Load and prepare training data\n",
    "X_raw, y = load_training_data(\"name_training_data.json\")\n",
    "vec = DictVectorizer(sparse=False)\n",
    "X_vec = vec.fit_transform(X_raw)\n",
    "\n",
    "# Train and save model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_vec, y)\n",
    "\n",
    "joblib.dump(clf, \"name_model.joblib\")\n",
    "joblib.dump(vec, \"name_vectorizer.joblib\")\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_vec)\n",
    "print(classification_report(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "303acf84-6c77-459c-af20-2ded17dfa719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.             ‚Üí NAME prob: 0.98 (Non-name: 0.02)\n",
      "CHARITHA        ‚Üí NAME prob: 0.68 (Non-name: 0.32)\n",
      "SRI             ‚Üí NAME prob: 0.82 (Non-name: 0.18)\n",
      "KUNAPAREDDY     ‚Üí NAME prob: 0.94 (Non-name: 0.06)\n",
      "5               ‚Üí NAME prob: 0.00 (Non-name: 1.00)\n",
      "years           ‚Üí NAME prob: 0.00 (Non-name: 1.00)\n",
      "Predicted Name: Dr. CHARITHA SRI KUNAPAREDDY\n"
     ]
    }
   ],
   "source": [
    "import joblib  # or just `import joblib` if you're using `joblib` directly\n",
    "\n",
    "def predict_names(tokens):\n",
    "    clf = joblib.load(\"name_model.joblib\")       # Make sure this model supports predict_proba()\n",
    "    vec = joblib.load(\"name_vectorizer.joblib\")\n",
    "\n",
    "    features = [extract_features(tok, idx) for idx, tok in enumerate(tokens)]\n",
    "    X_vec = vec.transform(features)\n",
    "    probs = clf.predict_proba(X_vec)  # Get probabilities for each class\n",
    "\n",
    "    # Class 1 is usually the \"name\" label\n",
    "    for tok, prob in zip(tokens, probs):\n",
    "        print(f\"{tok:<15} ‚Üí NAME prob: {prob[1]:.2f} (Non-name: {prob[0]:.2f})\")\n",
    "\n",
    "    # Get the most confident name predictions\n",
    "    name_tokens = [tok for tok, prob in zip(tokens, probs) if prob[1] > 0.5]\n",
    "    return \" \".join(name_tokens)\n",
    "\n",
    "# Example usage\n",
    "tokens = [\"Dr.\", \"CHARITHA\", \"SRI\", \"KUNAPAREDDY\", \"5\", \"years\"]\n",
    "print(\"Predicted Name:\", predict_names(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8aa0e80-948b-4633-9736-e31099e2598b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for mismatched tokens and labels...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def find_label_mismatches(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(\"Checking for mismatched tokens and labels...\\n\")\n",
    "    for i, item in enumerate(data):\n",
    "        tokens = item.get(\"tokens\", [])\n",
    "        labels = item.get(\"labels\", [])\n",
    "        if len(tokens) != len(labels):\n",
    "            print(f\"‚ùå Mismatch at index {i}:\")\n",
    "            print(f\"  Tokens ({len(tokens)}): {tokens}\")\n",
    "            print(f\"  Labels ({len(labels)}): {labels}\\n\")\n",
    "\n",
    "find_label_mismatches(\"name_training_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "84ca69fd-5872-4856-ade1-74e8853ad4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-compact JSON written to semi_compact_output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'extra_ner_name_college_dataset.json'       # your unstructured file\n",
    "output_file = 'semi_compact_output.json'\n",
    "\n",
    "# Load the existing pretty-printed JSON\n",
    "with open(input_file, 'r') as infile:\n",
    "    data = json.load(infile)  # This must be a list of dicts\n",
    "\n",
    "# Custom writing logic\n",
    "with open(output_file, 'w') as outfile:\n",
    "    outfile.write('[\\n')\n",
    "    for idx, item in enumerate(data):\n",
    "        json_str = json.dumps(item, indent=None)\n",
    "        # Pretty-print with keys on separate lines\n",
    "        parsed = json.loads(json_str)\n",
    "        outfile.write('  {\\n')\n",
    "        for i, (k, v) in enumerate(parsed.items()):\n",
    "            comma = ',' if i < len(parsed) - 1 else ''\n",
    "            line = f'    \"{k}\": {json.dumps(v)}{comma}\\n'\n",
    "            outfile.write(line)\n",
    "        comma = ',' if idx < len(data) - 1 else ''\n",
    "        outfile.write(f'  }}{comma}\\n')\n",
    "    outfile.write(']\\n')\n",
    "\n",
    "print(f\"Semi-compact JSON written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcc6ad37-e734-41c8-b426-3521f340ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Force PyTorch to avoid Keras issues\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"dslim/bert-base-NER\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "def extract_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "\n",
    "    name = []\n",
    "    education = []\n",
    "    skills = []\n",
    "\n",
    "    for ent in entities:\n",
    "        label = ent[\"entity_group\"]\n",
    "        word = ent[\"word\"]\n",
    "\n",
    "        if label == \"PER\":\n",
    "            name.append(word)\n",
    "        elif label == \"ORG\":\n",
    "            education.append(word)\n",
    "        elif label == \"MISC\":\n",
    "            skills.append(word)\n",
    "\n",
    "    return {\n",
    "        \"name\": \" \".join(name),\n",
    "        \"education_orgs\": list(set(education)),\n",
    "        \"skills\": list(set(skills))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdccbe3-04bf-4834-ba69-2ea3fab39d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '', 'education_orgs': ['MCA', 'Tri', '##dhartha Engineering College', 'Learning', 'FastAP', 'VR', 'Mahila Degree College', 'CHARITHA SRI KUNAP'], 'skills': ['SQL', 'Machine', 'Pandas', 'Python']}\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "CHARITHA SRI KUNAPAREDDY\n",
    "Email: charitha@example.com\n",
    "\n",
    "Education:\n",
    "MCA from VR Siddhartha Engineering College, Andhra Pradesh\n",
    "B.Sc from Triveni Mahila Degree College\n",
    "\n",
    "Skills: Python, SQL, Machine Learning, Pandas, FastAPI\n",
    "\"\"\"\n",
    "\n",
    "result = extract_entities(sample_text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591e60ab-6874-4ed9-b83f-0b024175e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üîç Results from: BERT-NER (dslim)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARITHA SRI KUN          ‚Üí ORG\n",
      "MCA                       ‚Üí ORG\n",
      "VR Siddhartha Engineering College ‚Üí ORG\n",
      "Triveni Mahila Degree College ‚Üí ORG\n",
      "Python                    ‚Üí MISC\n",
      "S                         ‚Üí MISC\n",
      "CS                        ‚Üí MISC\n",
      "Java                      ‚Üí MISC\n",
      "FastAP                    ‚Üí MISC\n",
      "Salesforce Catalyst       ‚Üí ORG\n",
      "\n",
      "==============================\n",
      "üîç Results from: RoBERTa-NER (Jean-Baptiste)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Jean-Baptiste--roberta-large-ner-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARITHA SRI              ‚Üí PER\n",
      " KUNAPAREDDY              ‚Üí ORG\n",
      " charitha                 ‚Üí PER\n",
      "sri                       ‚Üí PER\n",
      " VR Siddhartha Engineering College ‚Üí ORG\n",
      " Triveni Mahila Degree College ‚Üí ORG\n",
      " Python                   ‚Üí MISC\n",
      " SQL                      ‚Üí MISC\n",
      " HTML                     ‚Üí MISC\n",
      " CSS                      ‚Üí MISC\n",
      " Java                     ‚Üí MISC\n",
      " FastAPI                  ‚Üí MISC\n",
      " Salesforce Catalyst\n",
      "     ‚Üí ORG\n",
      "\n",
      "==============================\n",
      "üîç Results from: Multilingual-BERT (Davlan)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--Davlan--bert-base-multilingual-cased-ner-hrl. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCA                       ‚Üí ORG\n",
      "VR Siddhartha Engineering College ‚Üí ORG\n",
      "BS                        ‚Üí ORG\n",
      "Triveni Mahila Degree College ‚Üí ORG\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Define models to compare\n",
    "model_names = {\n",
    "    \"BERT-NER (dslim)\": \"dslim/bert-base-NER\",\n",
    "    \"RoBERTa-NER (Jean-Baptiste)\": \"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    \"Multilingual-BERT (Davlan)\": \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
    "}\n",
    "\n",
    "# Sample resume snippet (can be longer)\n",
    "resume_text = \"\"\"\n",
    "CHARITHA SRI KUNAPAREDDY\n",
    "Email: charitha.sri@example.com\n",
    "Phone: 9876543210\n",
    "\n",
    "Education:\n",
    "MCA - VR Siddhartha Engineering College\n",
    "BSc - Triveni Mahila Degree College\n",
    "\n",
    "Skills: Python, SQL, HTML, CSS, Java, FastAPI\n",
    "\n",
    "Experience:\n",
    "Intern at Salesforce Catalyst\n",
    "\"\"\"\n",
    "\n",
    "# Run inference across all models\n",
    "def extract_entities_from_model(model_name):\n",
    "    print(f\"\\n{'='*30}\\nüîç Results from: {model_name}\\n{'='*30}\")\n",
    "    ner = pipeline(\n",
    "        \"token-classification\",\n",
    "        model=model_names[model_name],\n",
    "        aggregation_strategy=\"simple\",\n",
    "        framework=\"pt\"\n",
    "    )\n",
    "    entities = ner(resume_text)\n",
    "    for ent in entities:\n",
    "        print(f\"{ent['word']:25} ‚Üí {ent['entity_group']}\")\n",
    "\n",
    "# Compare all models\n",
    "for model in model_names:\n",
    "    extract_entities_from_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6f434c5-3e53-48dd-8565-3615a083b73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# ---------- Load Hugging Face NER Model ----------\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# ---------- Extract Text from PDF ----------\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "# ---------- Split Resume into Sections ----------\n",
    "def split_resume_sections(text):\n",
    "    sections = {}\n",
    "    current = \"general\"\n",
    "    sections[current] = []\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        clean = line.strip()\n",
    "        if not clean:\n",
    "            continue\n",
    "\n",
    "        # Match section headers\n",
    "        if re.match(r\"^(education|education details|experience|skills|projects|summary|objective|certifications?)[:\\s]*$\", clean.lower()):\n",
    "            current = clean.lower().strip(\": \")\n",
    "            sections[current] = []\n",
    "        else:\n",
    "            sections.setdefault(current, []).append(clean)\n",
    "    print(sections)\n",
    "    return sections\n",
    "\n",
    "# ---------- Apply NER to Header + Education ----------\n",
    "def extract_name_and_education_sectional(text):\n",
    "    sections = split_resume_sections(text)\n",
    "    \n",
    "    header_text = \"\\n\".join(sections.get(\"general\", [])[:5])  # top lines only\n",
    "    edu_text = \"\\n\".join(sections.get(\"education\", []))\n",
    "\n",
    "    entities_header = ner_pipeline(header_text)\n",
    "    entities_edu = ner_pipeline(edu_text)\n",
    "\n",
    "    name_tokens = [e['word'] for e in entities_header if e[\"entity_group\"] == \"PER\"]\n",
    "    org_tokens = [e['word'] for e in entities_edu if e[\"entity_group\"] == \"ORG\"]\n",
    "\n",
    "    name = \" \".join(name_tokens).strip()\n",
    "    education = list(set(org_tokens))\n",
    "\n",
    "    return name, education\n",
    "\n",
    "# ---------- Run the Pipeline ----------\n",
    "def parse_pdf_resume(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    name, education = extract_name_and_education_sectional(text)\n",
    "    \n",
    "    print(\"üë§ Name:\", name)\n",
    "    print(\"üéì Education Orgs:\", education)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afefd9b-db9f-4c33-9d2e-99072a0c0e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'general': [], 'summary': ['Languages: English, Hindi, Telugu.', 'Masters of Computer Applications - MCA', 'VR Siddhartha Engineering College', 'Pursuing MCA', 'Aug 2020 - Mar 2024', 'Bachelors of Science - Mathematics, Statistics Computer Science', 'Triveni Mahila Degree College', 'CGPA : 7.0', 'Salesforce developer catalyst', 'Learned new emerging technologies currently used in the industry, focusing on job-oriented skills.', 'Gained insights into the working environment of the industry and its specific requirements.', 'Developed essential skills such as communication, interpersonal abilities, and other critical skills', 'necessary for the job interview process.', 'Saloon management system website', 'Developed an intuitive online web-based management application with appointment scheduling', 'functionality, enabling users to easily access information about services, offers, and stylists. The', 'system allows users to seamlessly book appointments with their preferred stylist and cancel them', 'anytime. The application is designed for ease of use, providing an efficient experience for both', 'customers and salon management.', 'Technologies Used: HTML, CSS, JavaScript, PHP, MySQL', 'My most valued assets include strong communication, creative problem-solving, and organizational skills,', 'which I am eager to apply in a software development role at your company. As a dedicated and collaborative', 'team player, I bring a proactive approach to developing innovative solutions and can be relied upon to', \"contribute effectively to achieving your organization's objectives\", 'CHARITHA SRI KUNAPAREDDY', 'Pappulamillu Center., Vijayawada |  charithasri.kunapareddy@gmail.com | charitha_sri_linkedin', 'SOFTWARE DEVELOPER', 'TECHNICAL SKILLS', 'PROJECTS & INTERNSHIPS'], 'education': ['ADDITIONAL INFORMATION', 'Python', 'C', 'C++', 'HTML,CSS, PHP', 'MySQL', 'Computer Networks', 'Team Player', 'Communication', 'Adaptability', 'Aug 2024 - Present', 'Intermediate - Maths, Physics, Chemistry', 'Govt. Junior College', 'CGPA : 8.9', 'Aug 2018 - Mar 2020']}\n",
      "üë§ Name: \n",
      "üéì Education Orgs: [' Junior College', 'Govt']\n"
     ]
    }
   ],
   "source": [
    "parse_pdf_resume(\"Charitha Resume.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f689c6dd-7b1e-4d80-84f0-b76bddc7a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "nickmuchi/roberta-base-resume-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/nickmuchi/roberta-base-resume-ner/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\transformers\\utils\\hub.py:470\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 470\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:1008\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:1115\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1115\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:1645\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1641\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m   1642\u001b[0m ):\n\u001b[0;32m   1643\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:1533\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1533\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:1450\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1450\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    287\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    288\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    289\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[1;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m     )\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-685963d9-2d83b726196c43b11e727881;4097c1a3-4d8b-4c80-90a7-69b0746877c6)\n\nRepository Not Found for url: https://huggingface.co/nickmuchi/roberta-base-resume-ner/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# ---------- Load NER Pipeline ----------\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m ner_pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnickmuchi/roberta-base-resume-ner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     97\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# ---------- Extract Name and Education via NER ----------\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_name_and_education_from_json\u001b[39m(resume_json):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\transformers\\pipelines\\__init__.py:812\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m--> 812\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m    813\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    814\u001b[0m         CONFIG_NAME,\n\u001b[0;32m    815\u001b[0m         _raise_exceptions_for_gated_repo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    816\u001b[0m         _raise_exceptions_for_missing_entries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    817\u001b[0m         _raise_exceptions_for_connection_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    818\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mmodel_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    819\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\transformers\\utils\\hub.py:312\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    255\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    256\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    259\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    313\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf310\\lib\\site-packages\\transformers\\utils\\hub.py:502\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[1;32m--> 502\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    506\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    507\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[0;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    511\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: nickmuchi/roberta-base-resume-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "# Download stopwords\n",
    "download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# ---------- Header Group Mapping ----------\n",
    "HEADER_GROUPS = {\n",
    "    \"about\": [\"about\", \"summary\", \"about me\", \"objective\"],\n",
    "    \"education\": [\"education\", \"education details\"],\n",
    "    \"skills\": [\"technical skills\", \"skills\", \"expertise\", \"strengths and expertise\"],\n",
    "    \"experience\": [\"experience\", \"professional experience\", \"projects\", \"project experience\"],\n",
    "    \"additional_information\": [\"additional information\", \"more about me\", \"certifications\"]\n",
    "}\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text.strip().lower())\n",
    "\n",
    "def match_standard_header(text):\n",
    "    norm = normalize_text(text)\n",
    "    for std_key, variants in HEADER_GROUPS.items():\n",
    "        for variant in variants:\n",
    "            if norm.startswith(variant):\n",
    "                return std_key\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    filtered = [w for w in words if w.lower() not in STOP_WORDS]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# ---------- Extract and Structure Resume Data ----------\n",
    "def extract_sections_as_json(pdf_path, spacing_threshold=20):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[0]\n",
    "\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    blocks = [b for b in blocks if b[4].strip()]\n",
    "    blocks.sort(key=lambda b: b[1])  # top to bottom\n",
    "\n",
    "    # Topmost block\n",
    "    topmost_block = min(blocks, key=lambda b: b[1])\n",
    "    result = {\"header\": topmost_block[4].strip()}\n",
    "\n",
    "    # Group into visual sections\n",
    "    sections = []\n",
    "    current_section = []\n",
    "    prev_y1 = None\n",
    "\n",
    "    for block in blocks:\n",
    "        x0, y0, x1, y1, text, *_ = block\n",
    "        if prev_y1 is not None and (y0 - prev_y1) > spacing_threshold:\n",
    "            if current_section:\n",
    "                sections.append(current_section)\n",
    "                current_section = []\n",
    "        current_section.append(block)\n",
    "        prev_y1 = y1\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(current_section)\n",
    "\n",
    "    # Flatten and scan for headers\n",
    "    all_blocks = [b for section in sections for b in section]\n",
    "    all_blocks.sort(key=lambda b: b[1])\n",
    "\n",
    "    current_key = None\n",
    "    section_data = {}\n",
    "\n",
    "    for block in all_blocks:\n",
    "        raw_text = block[4].strip()\n",
    "        std_key = match_standard_header(raw_text)\n",
    "        if std_key:\n",
    "            current_key = std_key\n",
    "            if current_key not in section_data:\n",
    "                section_data[current_key] = []\n",
    "        elif current_key:\n",
    "            section_data[current_key].append(raw_text)\n",
    "\n",
    "    # Clean and assign\n",
    "    for key, texts in section_data.items():\n",
    "        full_text = \" \".join(texts)\n",
    "        cleaned = clean_text(full_text)\n",
    "        result[key] = cleaned\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- Load NER Pipeline ----------\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# ---------- Extract Name and Education via NER ----------\n",
    "def extract_name_and_education_from_json(resume_json):\n",
    "    header_text = resume_json.get(\"header\", \"\")\n",
    "    print(header_text)\n",
    "    education_text = resume_json.get(\"education\", \"\")\n",
    "\n",
    "    entities_header = ner_pipeline(header_text)\n",
    "    \n",
    "    entities_edu = ner_pipeline(education_text)\n",
    "\n",
    "    name_tokens = [e['word'] for e in entities_header if e[\"entity_group\"] == \"PER\"]\n",
    "    print(name_tokens)\n",
    "    org_tokens = [e['word'] for e in entities_edu if e[\"entity_group\"] == \"ORG\"]\n",
    "\n",
    "    name = \" \".join(name_tokens).strip()\n",
    "    education_orgs = list(set(org_tokens))\n",
    "\n",
    "    return name, education_orgs\n",
    "\n",
    "# ---------- Main Pipeline Function ----------\n",
    "def parse_pdf_resume(pdf_path):\n",
    "    resume_json = extract_sections_as_json(pdf_path)\n",
    "    name, education_orgs = extract_name_and_education_from_json(resume_json)\n",
    "\n",
    "    print(\"üßæ Parsed Resume Data (JSON):\")\n",
    "    print(resume_json)\n",
    "    print(\"\\nüë§ Name:\", name)\n",
    "    print(\"üéì Education Orgs:\", education_orgs)\n",
    "    return {\n",
    "        \"parsed_json\": resume_json,\n",
    "        \"name\": name,\n",
    "        \"education_orgs\": education_orgs\n",
    "    }\n",
    "\n",
    "# ---------- Example Usage ----------\n",
    "result = parse_pdf_resume(\"Mayukha Resume.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f815208f-5d08-4338-9169-0161e3a9d7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9568997),\n",
       "  'word': ' Pappulamillu Center',\n",
       "  'start': 0,\n",
       "  'end': 19},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.99787545),\n",
       "  'word': ' Vijayawada',\n",
       "  'start': 21,\n",
       "  'end': 31},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.96347255),\n",
       "  'word': 'MAYUKHA KUNAPAREDDY',\n",
       "  'start': 40,\n",
       "  'end': 59}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de9f606c-9eb3-4dc1-9b87-349d8a9deedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loading NER models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßæ Structured Resume JSON:\n",
      "{'header': 'Pappulamillu Center, Vijayawada, 520007\\nMAYUKHA KUNAPAREDDY', 'skills': 'Power Systems Team Leadership Chip Designing Circuit Designing Communication Control Systems Electronics Time Management Stimulation Software Renewable Energy Systems Adaptibility AutoCad', 'education': 'Diploma Electrical Electronics Engineering Govt Polytechnic College Vijayawada August 2022 Present GPA 7 23 Active Participant Planning Execution College Events SSC G E C High School Patamata Vijayawada July 2021 May 2022 GPA 9 2 Actively engaged variety extracurricular activities including yoga essay writing competitions science fairs showcasing well rounded skill set enthusiasm persona l growth creative exploration', 'experience': 'PROJECT Implementation 3 phase Distribution Line Fault Detector Govt Polytechnic College Vijayawada study focuses detecting pole faults power distribution systems using real time monitoring advanced diagnostic techniques proposed approach enhances faul identification accuracy improving system reliability maintenance efficiency Trainee AP TRANSCO Gained hands experience operating equipment applying power system concepts practical scenarios demonstrating technical proficiency problem solving skills real worl situations', 'additional_information': 'Languages Telugu English Hindi Successfully completed Dakshin Bharat Hindi Prachar Sabha examinations Rashtra Bhasha level demonstrating proficiency Hindi language skills Hobbies Listening music drawing'}\n",
      "\n",
      "=== üì¶ MODEL COMPARISON ===\n",
      "üîπ Jean-Baptiste/roberta-large-ner-english\n",
      "  üë§ Name: MAYUKHA KUNAPAREDDY\n",
      "  üéì Education: [' SSC', ' Patamata Vijayawada', ' Govt Polytechnic College Vijayawada']\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from transformers import pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "# Download stopwords\n",
    "download('stopwords')\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# ---------- Header Group Mapping ----------\n",
    "HEADER_GROUPS = {\n",
    "    \"about\": [\"about\", \"summary\", \"about me\", \"objective\"],\n",
    "    \"education\": [\"education\", \"education details\"],\n",
    "    \"skills\": [\"technical skills\", \"skills\", \"expertise\", \"strengths and expertise\"],\n",
    "    \"experience\": [\"experience\", \"professional experience\", \"projects\", \"project experience\"],\n",
    "    \"additional_information\": [\"additional information\", \"more about me\", \"certifications\"]\n",
    "}\n",
    "\n",
    "# ---------- Helper Functions ----------\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text.strip().lower())\n",
    "\n",
    "def match_standard_header(text):\n",
    "    norm = normalize_text(text)\n",
    "    for std_key, variants in HEADER_GROUPS.items():\n",
    "        for variant in variants:\n",
    "            if norm.startswith(variant):\n",
    "                return std_key\n",
    "    return None\n",
    "\n",
    "def clean_text(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    filtered = [w for w in words if w.lower() not in STOP_WORDS]\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "# ---------- Extract and Structure Resume Data ----------\n",
    "def extract_sections_as_json(pdf_path, spacing_threshold=20):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc[0]\n",
    "\n",
    "    blocks = page.get_text(\"blocks\")\n",
    "    blocks = [b for b in blocks if b[4].strip()]\n",
    "    blocks.sort(key=lambda b: b[1])\n",
    "\n",
    "    topmost_block = min(blocks, key=lambda b: b[1])\n",
    "    result = {\"header\": topmost_block[4].strip()}\n",
    "\n",
    "    sections = []\n",
    "    current_section = []\n",
    "    prev_y1 = None\n",
    "\n",
    "    for block in blocks:\n",
    "        x0, y0, x1, y1, text, *_ = block\n",
    "        if prev_y1 is not None and (y0 - prev_y1) > spacing_threshold:\n",
    "            if current_section:\n",
    "                sections.append(current_section)\n",
    "                current_section = []\n",
    "        current_section.append(block)\n",
    "        prev_y1 = y1\n",
    "\n",
    "    if current_section:\n",
    "        sections.append(current_section)\n",
    "\n",
    "    all_blocks = [b for section in sections for b in section]\n",
    "    all_blocks.sort(key=lambda b: b[1])\n",
    "\n",
    "    current_key = None\n",
    "    section_data = {}\n",
    "\n",
    "    for block in all_blocks:\n",
    "        raw_text = block[4].strip()\n",
    "        std_key = match_standard_header(raw_text)\n",
    "        if std_key:\n",
    "            current_key = std_key\n",
    "            if current_key not in section_data:\n",
    "                section_data[current_key] = []\n",
    "        elif current_key:\n",
    "            section_data[current_key].append(raw_text)\n",
    "\n",
    "    for key, texts in section_data.items():\n",
    "        full_text = \" \".join(texts)\n",
    "        cleaned = clean_text(full_text)\n",
    "        result[key] = cleaned\n",
    "\n",
    "    return result\n",
    "\n",
    "# ---------- Apply NER from Different Models ----------\n",
    "def extract_name_and_education_with_model(ner_pipeline, resume_json, label_fallback=True):\n",
    "    header_text = resume_json.get(\"header\", \"\")\n",
    "    education_text = resume_json.get(\"education\", \"\")\n",
    "\n",
    "    entities_header = ner_pipeline(header_text)\n",
    "    entities_edu = ner_pipeline(education_text)\n",
    "\n",
    "    name_tokens = [e['word'] for e in entities_header if e[\"entity_group\"] == \"PER\"]\n",
    "\n",
    "    if not name_tokens and label_fallback:\n",
    "        fallback_orgs = [\n",
    "            e['word'] for e in entities_header\n",
    "            if e[\"entity_group\"] == \"ORG\"\n",
    "            and e['word'].isupper()\n",
    "            and 1 <= len(e['word'].split()) <= 3\n",
    "        ]\n",
    "        name_tokens = fallback_orgs\n",
    "\n",
    "    name = \" \".join(name_tokens).strip()\n",
    "    org_tokens = [e['word'] for e in entities_edu if e[\"entity_group\"] == \"ORG\"]\n",
    "    education_orgs = list(set(org_tokens))\n",
    "\n",
    "    return name, education_orgs\n",
    "\n",
    "# ---------- Load Models ----------\n",
    "print(\"üîÅ Loading NER models...\")\n",
    "ner_roberta_general = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"Jean-Baptiste/roberta-large-ner-english\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "# ---------- Main Runner ----------\n",
    "def compare_models_on_resume(pdf_path):\n",
    "    resume_json = extract_sections_as_json(pdf_path)\n",
    "\n",
    "    name_general, edu_general = extract_name_and_education_with_model(ner_roberta_general, resume_json)\n",
    "   \n",
    "\n",
    "    print(\"üßæ Structured Resume JSON:\")\n",
    "    print(resume_json)\n",
    "    print(\"\\n=== üì¶ MODEL COMPARISON ===\")\n",
    "    print(\"üîπ Jean-Baptiste/roberta-large-ner-english\")\n",
    "    print(\"  üë§ Name:\", name_general)\n",
    "    print(\"  üéì Education:\", edu_general)\n",
    "\n",
    "   \n",
    "\n",
    "    return {\n",
    "        \"structured_json\": resume_json,\n",
    "        \"model_general\": {\"name\": name_general, \"education\": edu_general}\n",
    "    }\n",
    "\n",
    "# ---------- Example Usage ----------\n",
    "result = compare_models_on_resume(\"Mayukha Resume.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47924193-0d9d-47e8-b779-b9ba97df5264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf310)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
